{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install madgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fix_png(class_nums):\n",
    "    total_sample = 6\n",
    "    img_size = (128, 128)\n",
    "    root = \"images/hedged_images\"\n",
    "    background_src = \"images/sources/imagenet/data_VGG_label\"\n",
    "    hedge_mask_src = \"images/sources/hedge_masks\"\n",
    "                    \n",
    "    assert type(total_sample) == int\n",
    "    \n",
    "    # val / test / train\n",
    "    for dataset_type in os.listdir(root):\n",
    "        dataset_type_path = os.path.join(root, dataset_type)\n",
    "        \n",
    "        # 0 - 1000\n",
    "        for class_num in class_nums:\n",
    "            class_num_path = os.path.join(dataset_type_path, str(class_num))\n",
    "            \n",
    "            # ILSVRC2012_XXX_XXXXX\n",
    "            for img_id in os.listdir(class_num_path):\n",
    "                img_id_path = os.path.join(class_num_path, img_id)\n",
    "                \n",
    "                # 0.1 - 0.8\n",
    "                for density in os.listdir(img_id_path):\n",
    "                    if \"png\" in density:\n",
    "                        # background validation\n",
    "                        try:\n",
    "                            background_path = os.path.join(img_id_path, img_id+\".png\")\n",
    "                            background = Image.open(background_path)\n",
    "                        except:\n",
    "                            print(\"background file {} is corrupted/missed\".format(background_path))\n",
    "                            background_src_item = os.path.join(background_src, class_num, img_id)\n",
    "                            background = Image.open(background_src_item+\".JPEG\")\n",
    "                            background.convert('RGB').save(background_src_item+\".png\")\n",
    "                    \n",
    "                    else:\n",
    "                        density_path = os.path.join(img_id_path, density)\n",
    "                        # 0-5\n",
    "                        for i in range(total_sample):\n",
    "                            \n",
    "                            # hedge_mask/img validation\n",
    "                            hedge_mask_path = os.path.join(density_path, str(i)+\"_hedge.png\")\n",
    "                            img_path = os.path.join(density_path, str(i)+\".png\")\n",
    "                            \n",
    "                            try:\n",
    "                                Image.open(hedge_mask_path)\n",
    "                                Image.open(img_path)\n",
    "                            except:\n",
    "                                print(\"corrupted/miss file: \\n image file {} \\n hedge file {}\".format(img_path, hedge_mask_path))\n",
    "                                # randomly pick one new hedge file from src\n",
    "\n",
    "                                hedge_root = os.path.join(hedge_mask_src, density)\n",
    "\n",
    "                                hedge_mask_code = np.random.choice(os.listdir(hedge_root), size=1)[0]\n",
    "                                hedge_mask_path = os.path.join(hedge_root, hedge_mask_code)\n",
    "                                hedge_mask = Image.open(hedge_mask_path)\n",
    "\n",
    "                                background_path = os.path.join(img_id_path, img_id+\".png\")\n",
    "                                background = Image.open(background_path)\n",
    "                                \n",
    "                                # generate hedged image and hedge mask then save them\n",
    "                                hedged_image, hedge_mask = generate_single_hedged_image(background, hedge_mask, img_size)\n",
    "                                hedged_image.convert('RGB').save(img_path)\n",
    "                                hedge_mask.convert('RGB').save(hedge_mask_path)\n",
    "                                \n",
    "if multi_processing:\n",
    "    pool = Pool(cpus)\n",
    "    print(\"multiprocessing will be run with {} threads\".format(cpus))\n",
    "    data = range(1000)\n",
    "    pool.map(fix_png, [data[x:x+250] for x in range(0, 1000, 250)])\n",
    "                    \n",
    "\n",
    "# def fix_png():\n",
    "#     path = \"/home/jupyter/src/images/hedged_images\"\n",
    "#     for dataset_type in os.listdir(path):\n",
    "#         dataset_type_path = os.path.join(path, dataset_type)\n",
    "#         for class_num in tqdm(os.listdir(dataset_type_path)):\n",
    "#             class_num_path = os.path.join(dataset_type_path, class_num)\n",
    "#             for img_id in os.listdir(class_num_path):\n",
    "#                 img_id_path = os.path.join(class_num_path, img_id)\n",
    "#                 os.system('mogrify *.png')                \n",
    "#                 for density in os.listdir(img_id_path):\n",
    "#                     density_path = os.path.join(img_id_path, density)\n",
    "#                     if not \"png\" in density_path:\n",
    "#                         os.chdir(density_path)\n",
    "#                         os.system('mogrify *.png')                \n",
    "                    \n",
    "                    \n",
    "#     os.chdir(\"/home/jupyter/src\")\n",
    "\n",
    "# fix_png()\n",
    "\n",
    "\n",
    "# image file /home/jupyter/src/images/hedged_images/train/743/ILSVRC2012_val_00031944/./0.png \n",
    "# or hedge file /home/jupyter/src/images/hedged_images/train/743/ILSVRC2012_val_00031944/./0_hedge.png is corrupted/missed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yK2WJm8olSb"
   },
   "source": [
    "# TODO: \n",
    "1. deal with logger object (local -> global), otherwise logger will echo itself printing repeated messages\n",
    "\n",
    "2. make sure all file names are stored in same format (e.g. png)\n",
    "\n",
    "3. check torch.cuda memory allocation (before 11 total 3 reserved then run out of memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YllrVcESJAyG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from google.colab import drive\n",
    "\n",
    "root = \"\"\n",
    "\n",
    "# select functions to run\n",
    "multi_processing = True\n",
    "generate_hedge_masks = False\n",
    "generate_hedge_image = False\n",
    "validate_dataset = False\n",
    "train_flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CnYqnyiATbX"
   },
   "source": [
    "# Check CPU/GPU status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HrmhhFgAGX8",
    "outputId": "3c075f5a-b710-434c-b2bb-2d5c337d5741",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vul0gzyOB4gv",
    "outputId": "5868559c-fa3f-4a6f-f2c0-413f2776878d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DH5D8bWvXRnw",
    "outputId": "8ce81c36-4117-4de1-c5d6-65c2bb8425eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "p = subprocess.Popen('df -h', shell=True, stdout=subprocess.PIPE)\n",
    "print(str(p.communicate()[0], 'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyh8r4vzeLbY"
   },
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9G-XL97IO7Jd",
    "outputId": "7db51588-8f91-4d2f-8da7-f5f68fdf34e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Process, Pool\n",
    "\n",
    "cpus = multiprocessing.cpu_count()\n",
    "\n",
    "def split_data_into_batches(data, number_of_batch):\n",
    "    portion_per_batch = int(len(data)/number_of_batch)\n",
    "    batches = []\n",
    "    for i in range(0, len(data), portion_per_batch):\n",
    "        new_batch = data[i:i+portion_per_batch]\n",
    "        batches.append(new_batch)\n",
    "    return np.array(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnW12ZrjA10-"
   },
   "source": [
    "# Config (path related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhiuLUTfA3qf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # sets device for model and PyTorch tensors\n",
    "print_freq = 100  # print training/validation stats  every __ batches\n",
    "\n",
    "# madgrad optimal lr is 0.001 (1e-3)\n",
    "\n",
    "def parse_args():\n",
    "    debug = False\n",
    "    load_path_from_last_checkpoint = \"checkpoint/recurrent_uncertainty_unet (madgrad+onecycle+no_dropout+512_complex+luv)/ep400(lr=0.001)/checkpoint.tar\"\n",
    "    # pretrained_UNet = \"checkpoint/unet(madgrad+onecycle+no_dropout+512_complex)/ep200(lr=0.001)/BEST_checkpoint.tar\"\n",
    "    pretrained_UNet = \"checkpoint/unet (madgrad+onecycle+no_dropout+512_complex+luv)/ep400(lr=0.0001)/BEST_checkpoint.tar\"    \n",
    "    # pretrained_Uncertainty_UNet = \"checkpoint/uncertainty_unet(madgrad+ReduceLROnPlateau+no_dropout+512_complex)/ep700(lr=0.0001)/BEST_checkpoint.tar\"\n",
    "    pretrained_Uncertainty_UNet = \"checkpoint/uncertainty_unet (madgrad+onecycle+no_dropout+512_complex+luv)/ep400(lr=1e-05)/BEST_checkpoint.tar\"\n",
    "    \n",
    "    model_type = \"recurrent_uncertainty_unet\"\n",
    "    \n",
    "    if model_type == \"unet\" or model_type == \"uncertainty_unet\":\n",
    "        masked_img_per_item = 1\n",
    "        in_channel = 3\n",
    "    elif model_type == \"recurrent_uncertainty_unet\":\n",
    "        masked_img_per_item = 2\n",
    "        in_channel = 9\n",
    "    else:\n",
    "        print(\"model type not supported\")\n",
    "    \n",
    "    optimizer = \"madgrad\"    \n",
    "    \n",
    "    change_scheduler = False\n",
    "    scheduler = \"onecycle\"\n",
    "\n",
    "    caption = \"no_dropout+512_complex+luv\"\n",
    "    model_name = \"{} ({}+{}+{})\".format(model_type, optimizer, scheduler, caption)\n",
    "    \n",
    "    change_lr = False\n",
    "    lr = 1e-3\n",
    "    \n",
    "    end_epoch = 600\n",
    "    early_stop = 999\n",
    "    batch_size = 64\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Train u-net')\n",
    "    \n",
    "    parser.add_argument('--debug', type=bool, default=debug, help='debug mode, reduce the size of dataset to run through train/val process faster')\n",
    "    parser.add_argument('--tensorboard-fileName', type=str, default=\"{} - ep{}(lr={})\".format(model_name, end_epoch, lr), help='tensorboard runs file name')\n",
    "    \n",
    "    # model related\n",
    "    parser.add_argument('--unet-type', type=str, default=model_type, help='unet/uncertainty_unet/recurrent_uncertainty_unet')\n",
    "    parser.add_argument('--checkpoint-load-path', type=str, default=load_path_from_last_checkpoint, help='checkpoint to pick up from last training session')\n",
    "    parser.add_argument('--checkpoint-save-path', type=str, default=\"checkpoint/{}/ep{}(lr={})\".format(model_name, end_epoch, lr), help='checkpoint path')\n",
    "    parser.add_argument('--in-channel', type=int, default=in_channel, help='channel count for model input')\n",
    "    parser.add_argument('--out-channel', type=int, default=3, help='channel count for model output (prediction)')\n",
    "    parser.add_argument('--out-uncertainty-channel', type=int, default=3, help='channel count for model output (uncertainty)')    \n",
    "\n",
    "    # optimizer related\n",
    "    parser.add_argument('--optimizer', type=str, default=optimizer, help='optimizer(adam/SGD/madgrad)')\n",
    "    parser.add_argument('--lr', type=float, default=lr, help='start learning rate')\n",
    "    parser.add_argument('--end-epoch', type=int, default=end_epoch, help='training epoch size.')\n",
    "    parser.add_argument('--change-lr', type=bool, default=change_lr, help='checkpoint')\n",
    "    parser.add_argument('--clip-val', type=float, default=10, help='gradient clip value to prevent gradient explosion')\n",
    "    #     SGD\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum, range=[0,1)')\n",
    "    parser.add_argument('--weight-decay', type=float, default=0, help='weight_decay L2 penalty')\n",
    "    parser.add_argument('--nesterov', type=bool, default=False, help='nesterov momentum')\n",
    "    #     madgrad\n",
    "    parser.add_argument('--eps', type=float, default=1e-6, help='Term added to the denominator outside of the root operation to improve numerical stability (default: 1e-6)')\n",
    "    \n",
    "    # early stopping\n",
    "    parser.add_argument('--early-stop-ep', type=int, default=early_stop, help='ep to stay patient for no validation loss improvement, if exceed this ep, training stop automatically ')\n",
    "    \n",
    "    # scheduler\n",
    "    parser.add_argument('--scheduler', type=str, default=scheduler, help=\"None/ReduceLROnPlateau/exp/cosine/onecycle\")\n",
    "    parser.add_argument('--scheduler-step', type=int, default=5, help=\"n step until scheduler step and perform weight decay\")\n",
    "    parser.add_argument('--change-scheduler', type=bool, default=change_scheduler, help=\"change scheduler or not\")\n",
    "    parser.add_argument('--verbose', type=bool, default=True, help=\"verbose for scheduler (notification on changing lr)\")\n",
    "    #     onecycle\n",
    "    parser.add_argument('--max-lr', type=int, default=1e-4, help=\"Upper learning rate boundaries in the cycle for each parameter group\")\n",
    "    parser.add_argument('--total-steps ', type=int, default=None, help=\"The total number of steps in the cycle. Note that if a value is not provided here, then it must be inferred by providing a value for epochs and steps_per_epoch. Default: None\")\n",
    "    # parser.add_argument('--epochs', type=int, default=eps, help=\"The number of epochs to train for. This is used along with steps_per_epoch in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Default: None\")\n",
    "    # parser.add_argument('--steps-per-epoch', type=int, default=20, help=\"The number of steps per epoch to train for. This is used along with epochs in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Default: None\")\n",
    "    parser.add_argument('--pct-start', type=float, default=0.3, help=\"The percentage of the cycle (in number of steps) spent increasing the learning rate. Default: 0.3\")\n",
    "    parser.add_argument('--anneal-strategy', type=str, default='cos', help=\"Specifies the annealing strategy: “cos” for cosine annealing, “linear” for linear annealing. Default: ‘cos’\")\n",
    "    parser.add_argument('--cycle-momentum', type=bool, default=True, help=\"If True, momentum is cycled inversely to learning rate between ‘base_momentum’ and ‘max_momentum’. Default: True\")\n",
    "    parser.add_argument('--base-momentum', type=float, default=0.85, help=\"Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is ‘base_momentum’ and learning rate is ‘max_lr’. Default: 0.85\")\n",
    "    parser.add_argument('--max-momentum', type=float, default=0.95, help=\"Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is ‘max_momentum’ and learning rate is ‘base_lr’ Default: 0.95\")\n",
    "    parser.add_argument('--div-factor', type=float, default=25, help=\"Determines the initial learning rate via initial_lr = max_lr/div_factor Default: 25\")\n",
    "    parser.add_argument('--final-div-factor', type=float, default=1e4, help=\"Determines the minimum learning rate via min_lr = initial_lr/final_div_factor Default: 1e4\")\n",
    "    parser.add_argument('--three-phase', type=bool, default=False, help=\"If True, use a third phase of the schedule to annihilate the learning rate according to ‘final_div_factor’ instead of modifying the second phase (the first two phases will be symmetrical about the step indicated by ‘pct_start’).\")\n",
    "    #     cosineWarmRestart\n",
    "    parser.add_argument('--T-0', type=int, default=13, help=\"Number of iterations for the first restart.\")\n",
    "    parser.add_argument('--T-mult', type=int, default=2, help=\"A factor increases T1  after a restart. Default: 1\")\n",
    "    parser.add_argument('--eta-min', type=int, default=0, help=\"minimum learning rate for cosine curve\")\n",
    "    #     ReduceLROnPlateau\n",
    "    parser.add_argument('--factor', type=float, default=0.5, help=\"factor to decay weight, default=0.1\")\n",
    "    parser.add_argument('--min-lr', type=float, default=1e-12, help=\"minimum lr to be set, default=1e-4\")\n",
    "    parser.add_argument('--mode', type=str, default='min', help=\"choose min for loss, max for accuracy\")\n",
    "    parser.add_argument('--patience', type=int, default=10, help=\"stay patience for n epoch until decaying LR, default=10\")\n",
    "    \n",
    "    # dataset related\n",
    "    parser.add_argument('--train-path', type=str, default=\"images/hedged_images/train\", help='path to training data')\n",
    "    parser.add_argument('--val-path', type=str, default=\"images/hedged_images/val\", help='path to val data')\n",
    "    parser.add_argument('--test-path', type=str, default=\"images/hedged_images/test\", help='path to test data')\n",
    "    parser.add_argument('--dataset', type=str, default='static dataset', help='specify dynamic/static dataset')\n",
    "    parser.add_argument('--masked-img-per-item', type=int, default=masked_img_per_item, help='number of (masked image + ground truth) pairs output by the trainloader each iteration')\n",
    "    parser.add_argument('--batch-size', type=int, default=batch_size, help='batch size in each context')\n",
    "    parser.add_argument('--num-workers', type=int, default=multiprocessing.cpu_count(), help='number of cpu workers')\n",
    "    parser.add_argument('--pin-memory', type=bool, default=False, help='If True, the data loader will copy tensors into CUDA pinned memory before returning them, and data transfer to GPU can be faster')\n",
    "    parser.add_argument('--color-mode', type=str, default=\"LUV\", help='all loaded images will be convert to the color mode (RGB/HSV/LAB/LUV)')\n",
    "    \n",
    "    # image related\n",
    "    parser.add_argument('--image-size', type=tuple, default=(128, 128), help='input image size')\n",
    "    parser.add_argument('--min-density', type=float, default=0.1, help='max value for hedge density')\n",
    "    parser.add_argument('--max-density', type=float, default=0.9, help='max value for hedge density')\n",
    "    parser.add_argument('--sample-per-image', type=int, default=32, help='sample per image')\n",
    "    \n",
    "    # pretrained checkpoint\n",
    "    parser.add_argument('--pretrained-MSE-unet', type=str, default=pretrained_UNet, help='pretrained single frame MSE unet')\n",
    "    parser.add_argument('--pretrained-uncertainty-unet', type=str, default=pretrained_Uncertainty_UNet, help='pretrained uncertainty models for first frame')\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMejEidcFmjp",
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCWdah4yFkkH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "import cv2\n",
    "import numpy as np\n",
    "from random import getrandbits\n",
    "import torch\n",
    "import logging\n",
    "import urllib.error\n",
    "import urllib3.exceptions\n",
    "import json\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaojhk0iGVx3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_bool():\n",
    "    return bool(getrandbits(1))\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    except OSError:\n",
    "        print(\"OSError when creating directory {}\".format(path))\n",
    "\n",
    "def save_json(item, dest_file_name):\n",
    "    with open(dest_file_name, \"w\") as json_file:\n",
    "        json.dump(item, json_file)\n",
    "    \n",
    "def load_json(path):\n",
    "    return json.load(open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNNya4AOGOhs",
    "tags": []
   },
   "source": [
    "## logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1Z72QmdGPT0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_logger(path):\n",
    "    # prevent adding multiple handler causing logger to echo\n",
    "    if 'logger' in globals():\n",
    "        logger = logging.getLogger()\n",
    "        fh = logging.FileHandler(os.path.join(path, \"log_info.log\"))\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        logger.handlers[1] = fh\n",
    "        return logger\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)    \n",
    "    formatter = logging.Formatter(\"%(asctime)s %(levelname)s \\t%(message)s\")\n",
    "\n",
    "    # print in ide\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    # write to file\n",
    "    fh = logging.FileHandler(os.path.join(path, \"log_info.log\"))\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(fh)\n",
    "        \n",
    "    return logger\n",
    "\n",
    "args = parse_args()\n",
    "make_dir(args.checkpoint_save_path)\n",
    "logger = get_logger(args.checkpoint_save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parse_args()\n",
    "# to_tensor = torchvision.transforms.ToTensor()\n",
    "# path = \"images/hedged_images/train/115/ILSVRC2012_val_00018470/0.4/0.png\"\n",
    "\n",
    "# cv_im = cv2.imread(path)\n",
    "# pil_im = np.array(Image.open(path).convert(\"RGB\"))\n",
    "\n",
    "\n",
    "# # RGB_im = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "# # print(RGB_im == pil_im)\n",
    "\n",
    "# LAB_im = cv2.cvtColor(cv_im, cv2.COLOR_RGB2LAB)\n",
    "# RGB_im = cv2.cvtColor(LAB_im, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "\n",
    "# LAB_im_tensor = to_tensor(cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2LAB))\n",
    "# RGB_im_from_tensor = LAB_im_tensor.numpy()\n",
    "# RGB_im_from_tensor = np.transpose(RGB_im_from_tensor, (1, 2, 0))\n",
    "\n",
    "# RGB_im_from_tensor = cv2.cvtColor((RGB_im_from_tensor*255).astype('uint8'), cv2.COLOR_LAB2RGB)\n",
    "\n",
    "\n",
    "# plt.imshow(RGB_im)\n",
    "# plt.figure()\n",
    "# plt.imshow(RGB_im_from_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KX9D6vunFx4X"
   },
   "source": [
    "## plot/images related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20CUbvCdIIkC"
   },
   "source": [
    "plot graph - used to plot 2D curves\n",
    "imshow - used to display single image\n",
    "show_tensor_images - used to display multiple tensor images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnLdNhmiGOhi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pil_2_cv2(img):\n",
    "    return (img*255).astype('uint8')\n",
    "\n",
    "def lab_to_rgb(img):\n",
    "    return cv2.cvtColor(pil_2_cv2(img), cv2.COLOR_LAB2RGB)\n",
    "\n",
    "def luv_to_rgb(img):\n",
    "    return cv2.cvtColor(pil_2_cv2(img), cv2.COLOR_Luv2RGB)\n",
    "\n",
    "\n",
    "def plot_graph(title, x_label, y_label, data_batch_x, data_batch_y, legend, path, x_lim=None, y_lim=None):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    if x_lim:\n",
    "        plt.xlim(x_lim)\n",
    "        plt.ylim(y_lim)\n",
    "    for data_x, data_y, legend in zip(data_batch_x, data_batch_y, legend):\n",
    "        plt.plot(data_x, data_y, label=legend)\n",
    "    plt.legend()\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "def imshow(img, title=\"\", clip=False, colorbar=False):\n",
    "    np_img = img.numpy()\n",
    "    min_val = np.min(np_img)\n",
    "    max_val = np.max(np_img)\n",
    "    \n",
    "    # greyscale or RGB\n",
    "    if np_img.shape[0] == 1 or len(np_img.shape) == 2:\n",
    "        plt.imshow(np_img[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    else:\n",
    "        np_img = np.transpose(np_img, (1, 2, 0))\n",
    "            \n",
    "        if args.color_mode == \"HSV\":\n",
    "            np_img = hsv_to_rgb(np_img)\n",
    "        elif args.color_mode == \"LAB\":\n",
    "            np_img = lab_to_rgb(np_img)\n",
    "        elif args.color_mode == \"LUV\":\n",
    "            np_img = luv_to_rgb(np_img)       \n",
    "        \n",
    "        # clip / normalize\n",
    "        if clip and args.color_mode == \"RGB\":\n",
    "            np_img = np.clip(np_img, 0, 1)\n",
    "        else:\n",
    "            # adding 1/256 to avoid division by 0\n",
    "            np_img = (np_img - np.min(np_img)) / (np.ptp(np_img) + 1/256)        \n",
    "        \n",
    "        plt.imshow(np_img)\n",
    "\n",
    "    if colorbar:\n",
    "        plt.colorbar()    \n",
    "    plt.title(title)\n",
    "    plt.text(0, 150, \"{} range = [{min_val:.3f}, {max_val:.3f}]\".format(title, min_val=min_val, max_val=max_val))\n",
    "\n",
    "\n",
    "def show_tensor_images(tensor_imgs, title=[], path=None):\n",
    "    try:\n",
    "        plt.figure(figsize=(len(tensor_imgs) * 5, 5))\n",
    "        for i, tensor_img in enumerate(tensor_imgs):\n",
    "            plt.subplot(1, len(tensor_imgs), i + 1)\n",
    "            if title:\n",
    "                imshow(tensor_img.detach().cpu(), title[i])\n",
    "            else:\n",
    "                imshow(tensor_img.detach().cpu())\n",
    "        if path:\n",
    "            plt.savefig(path)\n",
    "        plt.show()\n",
    "    except urllib3.exceptions.HTTPError and urllib.error.HTTPError:\n",
    "        print(\"error: please close plots to free memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unEnNOMAFwDj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_images_to_imgdict(img_dict, image, name, add_rgb_map=False):\n",
    "    img_dict[name] = image\n",
    "    if add_rgb_map:\n",
    "        channel = [\"(R)\", \"(G)\", \"(B)\"]\n",
    "        for i, add_rgb in enumerate(channel):\n",
    "            img_dict[name + add_rgb] = np.transpose(image, (1, 2, 0))[:, :, i].reshape(1, 128, 128)\n",
    "\n",
    "\n",
    "def get_img_dict(masked_image, pred, ground_truth, uncertainty):\n",
    "    \"\"\" get a dict containing [key=name, val=[np images] ] to show images \"\"\"\n",
    "    img_dict = dict()\n",
    "\n",
    "    # turn argument directly into numpy arrays\n",
    "    masked_image = masked_image.detach().cpu()\n",
    "    pred = pred.detach().cpu()\n",
    "    ground_truth = ground_truth.detach().cpu()\n",
    "    uncertainty = uncertainty.detach().cpu()\n",
    "    \n",
    "    # loss function map\n",
    "    loss_map = np.exp(-uncertainty) * (pred - ground_truth) ** 2 + uncertainty\n",
    "    \n",
    "    # sigma map (std deviation)\n",
    "    sigma = np.exp(uncertainty / 2)\n",
    "\n",
    "    # abs error map\n",
    "    abs = torch.sqrt((pred - ground_truth) ** 2)  \n",
    "\n",
    "    # z-score image = abs/sig (we clip all negative values to 0)\n",
    "    z_score = abs / sigma\n",
    "    z_score = np.clip(z_score, 0, torch.max(z_score))\n",
    "\n",
    "    # store results in image dict so we could store the title as key and images as value\n",
    "    add_images_to_imgdict(img_dict, masked_image, \"masked image\", add_rgb_map=False)\n",
    "    add_images_to_imgdict(img_dict, pred, \"clipped prediction\", add_rgb_map=False)\n",
    "    add_images_to_imgdict(img_dict, ground_truth, \"ground truth\", add_rgb_map=False)\n",
    "    add_images_to_imgdict(img_dict, loss_map, \"loss map\", add_rgb_map=False)\n",
    "    # for the following images we add R, G and B map to observe each color channel\n",
    "    add_images_to_imgdict(img_dict, uncertainty, \"uncertainty map\", add_rgb_map=True)\n",
    "    add_images_to_imgdict(img_dict, sigma, \"sigma map\", add_rgb_map=True)\n",
    "    add_images_to_imgdict(img_dict, abs, \"abs map\", add_rgb_map=True)\n",
    "    add_images_to_imgdict(img_dict, z_score, \"z score\", add_rgb_map=True)\n",
    "\n",
    "    return img_dict\n",
    "\n",
    "\n",
    "def plot_img_dicts(img_dict, path):\n",
    "    try:\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for i, (name, img) in enumerate(img_dict.items()):\n",
    "            plt.subplot(int(np.ceil(len(img_dict) / 4)), 4, i + 1)\n",
    "            imshow(img, name, name.__contains__(\"clipped\"))\n",
    "\n",
    "        plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4)\n",
    "        if path:\n",
    "            plt.savefig(path)\n",
    "        plt.show()\n",
    "    except urllib3.exceptions.HTTPError and urllib.error.HTTPError:\n",
    "        print(\"error: please close plots to free memory\")\n",
    "\n",
    "\n",
    "def show_uncertainty_result(masked_image, pred, ground_truth, uncertainty, path=None):\n",
    "    img_dict = get_img_dict(masked_image, pred, ground_truth, uncertainty)\n",
    "    plot_img_dicts(img_dict, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVbQyGw-7aNB"
   },
   "source": [
    "## image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pkBLfyZ7ZhH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# because the ground truth was stored as its original size\n",
    "def resize_images(root_dir, size=(128, 128)):\n",
    "    \"\"\" used to correct the size for stored ground truth image files \"\"\"\n",
    "    for label in os.listdir(root_dir):\n",
    "        label_path = os.path.join(root_dir, label)\n",
    "        for image in os.listdir(label_path):\n",
    "            image_path = os.path.join(label_path, image, image + \".png\")\n",
    "            ground_truth_image = Image.open(image_path)\n",
    "            ground_truth_image = ground_truth_image.resize(size)\n",
    "            ground_truth_image.save(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqzI1CHHGFpF"
   },
   "source": [
    "## AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbyBM87lGEyC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    used to store and update the loss value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def update_meters(meters, values):\n",
    "    for meter, value in zip(meters, values):\n",
    "        meter.update(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHRoxQgJGJzw"
   },
   "source": [
    "## checkpoint related "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMPsAS0YGMMP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, epochs_since_improvement, u_net_model, optimizer, scheduler, val_loss, best_loss, is_best, checkpoint_path):\n",
    "    \n",
    "    if isinstance(u_net_model, torch.nn.DataParallel):\n",
    "        u_net_model = u_net_model.module\n",
    "    \n",
    "    state = {'epoch': epoch,\n",
    "             'epoch_since_improvement': epochs_since_improvement,\n",
    "             'u_net_model': u_net_model,\n",
    "             'optimizer': optimizer,\n",
    "             'scheduler': scheduler,\n",
    "             'val_loss': val_loss,\n",
    "             'best_loss': best_loss,\n",
    "             'is_best': is_best}\n",
    "    \n",
    "    file_name = os.path.join(checkpoint_path, 'checkpoint.tar')\n",
    "    torch.save(state, file_name)\n",
    "    if is_best:\n",
    "        logger.info(\"new best weight with validation loss = {}\".format(val_loss))\n",
    "        file_name = os.path.join(checkpoint_path, 'BEST_checkpoint.tar')\n",
    "        torch.save(state, file_name)\n",
    "    logger.info(\"check point saved\")\n",
    "\n",
    "\n",
    "def load_checkpoint_model(checkpoint):\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    return checkpoint['u_net_model']\n",
    "\n",
    "\n",
    "def choose_best_checkpoint(path):\n",
    "    \"\"\" get the best checkpoint based on validation loss from each epoch segments\"\"\"\n",
    "    min_loss = float('inf')\n",
    "    best_path = \"\"\n",
    "    for ep in os.listdir(path):\n",
    "        ep_path = os.path.join(path, ep, \"BEST_checkpoint.tar\")\n",
    "        checkpoint = torch.load(ep_path)\n",
    "        if checkpoint[\"loss\"] < min_loss:\n",
    "            min_loss = checkpoint[\"loss\"]\n",
    "            best_path = ep_path\n",
    "\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75PBxkVobLPn"
   },
   "source": [
    "# Hedge Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWfse0jZbRjl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from queue import Queue\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# from utils import random_bool, make_dir\n",
    "\n",
    "def rename_hedge_masks(src_path, start_num):\n",
    "    # used to rename hedge images based on their code (when merging hedge masks together into 1 directory)\n",
    "    try:\n",
    "        for density in enumerate(os.listdir(src_path)):\n",
    "            density_path = os.path.join(src_path, density)\n",
    "            for i, mask_path in enumerate(os.listdir(density_path)):\n",
    "                src_path = os.path.join(density_path, mask_path)\n",
    "                new_path = os.path.join(density_path, str(start_num+int(mask_path.split(\"_\")[0])) + \"_\" + mask_path.split(\"_\")[1])\n",
    "                os.rename(src_path, new_path)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.info(\"No such file or directory: \" + src_path)\n",
    "\n",
    "\n",
    "# rename_hedge_mask(\"/content/drive/MyDrive/Image Dehedger Project/images/sources/hedge masks\", 212)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA3yWnVxbZvw"
   },
   "source": [
    "## algorithm 1: generate transparent holes\n",
    "- generate holes in hedges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HimIzw5UcQ9e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_transparent_holes(hedge_img, hedge_density=0.3):\n",
    "    \"\"\"\n",
    "    :param hedge_img: PIL Image File\n",
    "    :param hedge_density: decide how dense is the hedge mask\n",
    "    :return hedge mask image\n",
    "    \"\"\"\n",
    "    hedge_img = hedge_img.convert('RGBA')  # RGBA (RGB Alpha)\n",
    "    pixels = hedge_img.getdata()  # convert to ImagingCore object (containing pixel values as tuples)\n",
    "\n",
    "    # get all the red channel, sort and find the threshold (1 - density)\n",
    "    r_pixels = [pixel[0] for pixel in pixels]\n",
    "    r_pixels.sort()\n",
    "    r_threshold = r_pixels[int((1 - hedge_density) * (len(r_pixels) - 1))]\n",
    "\n",
    "    # for each pixel if the red channel is smaller than the threshold, replace it with white pixel\n",
    "    new_pixels = []\n",
    "    for pixel in pixels:\n",
    "        if pixel[0] <= r_threshold:\n",
    "            new_pixels.append((255, 255, 255, 0))\n",
    "        else:\n",
    "            new_pixels.append(pixel)\n",
    "\n",
    "    # place the rearranged pixels into the image object\n",
    "    hedge_img.putdata(new_pixels)\n",
    "    return hedge_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr1H3giKnH1n"
   },
   "source": [
    "## algorithm 2: hedge mask denoising\n",
    "- remove patches of hedges that is too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6B5G0niXnHh0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_pixel_group(m_img, i, j, width, height, pixel_group_ids, pixel_group_id):\n",
    "    \"\"\"\n",
    "    an algorithm that \"spreads\" out from pixel (i, j) and check if its neighbor is non-hedge\n",
    "    if it is non-hedge then they would be allocated into the same group\n",
    "    :param m_img: image to be searched\n",
    "    :param i: index i of pixel\n",
    "    :param j: index j of pixel\n",
    "    :param width: width of image\n",
    "    :param height: height of image\n",
    "    :param pixel_group_ids: list that stores the group ID\n",
    "    :param pixel_group_id: id for the current pixel group\n",
    "    :return: area of current pixel group\n",
    "    \"\"\"\n",
    "    pixel_queue = Queue()  # pixels to be processed\n",
    "    pixel_queue.put((i, j))\n",
    "    pixel_group_ids[i, j] = pixel_group_id\n",
    "\n",
    "    area = 0\n",
    "\n",
    "    while not pixel_queue.empty():\n",
    "        (i, j) = pixel_queue.get()\n",
    "        area += 1\n",
    "        # for all neighbor (all pixels around current pixel)\n",
    "        for di in [-1, 0, 1]:\n",
    "            for dj in [-1, 0, 1]:\n",
    "                # make sure the index is within the scope of image\n",
    "                if 0 <= i + di < height and 0 <= j + dj < width:\n",
    "                    if (m_img[i + di, j + dj] > 0) and (pixel_group_ids[i + di, j + dj] == 0):\n",
    "                        pixel_group_ids[i + di, j + dj] = pixel_group_id\n",
    "                        # add all neighboring unvisited non-hedge pixel to the queue\n",
    "                        pixel_queue.put((i + di, j + dj))\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "def hedge_denoising(hedge_img, noise_area_threshold=200):\n",
    "    \"\"\"\n",
    "    remove all pixel segments with area smaller than MAX_NOISE_AREA\n",
    "    :param hedge_img: image to be de-noised\n",
    "    :param noise_area_threshold: the area threshold for the pixel group to remain in the image\n",
    "    :return: denoised hedge mask\n",
    "    \"\"\"\n",
    "    width, height = hedge_img.size\n",
    "    pixels = hedge_img.getdata()\n",
    "    m_img = np.asarray(pixels)[..., 3].reshape(height, width)\n",
    "\n",
    "    pixel_group_ids = np.zeros(m_img.shape).astype(int)  # store group ID for each non-hedge pixels\n",
    "    area = np.zeros(m_img.size).astype(int)  # stores area for each group ID\n",
    "\n",
    "    pixel_group_id = 0\n",
    "    new_pixels = []\n",
    "    for x in range(height):\n",
    "        for j in range(width):\n",
    "            # check if current pixel is non-hedge, if it is hedge, then just append\n",
    "            if m_img[x, j] > 0:\n",
    "                # if the pixel haven't been visited\n",
    "                if pixel_group_ids[x, j] == 0:\n",
    "                    pixel_group_id += 1\n",
    "                    area[pixel_group_id] = search_pixel_group(m_img, x, j, width, height, pixel_group_ids,\n",
    "                                                              pixel_group_id)\n",
    "\n",
    "                # if the current pixel belongs to a group with area smaller than noise_threshold, remove it\n",
    "                if area[pixel_group_ids[x, j]] < noise_area_threshold:\n",
    "                    new_pixels.append((255, 255, 255, 0))\n",
    "                else:\n",
    "                    new_pixels.append(pixels[len(new_pixels)])\n",
    "            else:\n",
    "                new_pixels.append(pixels[len(new_pixels)])\n",
    "\n",
    "    hedge_img.putdata(new_pixels)\n",
    "    return hedge_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEsfvs3lncRw"
   },
   "source": [
    "## algorithm 3: compute density\n",
    "- the density assigned in algorithm 1 will be affected by algorithm 2 since it removes parts of hedges that is too small, so the density have to be re-calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEOE_dxynwu_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_density(hedge_img):\n",
    "    \"\"\" compute the actual hedge density of the hedge mask\"\"\"\n",
    "    width, height = hedge_img.size\n",
    "    pixels = hedge_img.getdata()\n",
    "    m_img = np.asarray(pixels)[..., 3].reshape(height, width)\n",
    "    return np.sum(m_img > 0) / m_img.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWkx_5LUn2na"
   },
   "source": [
    "## generate hedge masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFLxy9Fp6s_1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_hedge_mask(hedge_img, hedge_density, noise_area_threshold):\n",
    "    \"\"\"\n",
    "    1) first generate holes with algorithm 1\n",
    "    2) then remove noises using algorithm 2\n",
    "    3) finally compute the hedge density\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    logger.info(\"step 1 - generate holes\")\n",
    "    hedge_mask = generate_transparent_holes(hedge_img, hedge_density)\n",
    "    logger.info(\"step 2 - denoising\")\n",
    "    hedge_mask = hedge_denoising(hedge_mask, noise_area_threshold)\n",
    "    logger.info(\"step 3 - density calculation\")\n",
    "    hedge_density = round(compute_density(hedge_mask), 1)\n",
    "    return hedge_mask, hedge_density\n",
    "\n",
    "\n",
    "def generate_all_hedge_masks(resource_dir, save_dir, temp_dir, min_density, max_density, density_step, noise_area_threshold, processed_count=0):\n",
    "    for i, item in enumerate(os.listdir(resource_dir)):\n",
    "        logger.info(\"processing image {}: {}\".format(i, item))\n",
    "        hedge_path = os.path.join(resource_dir, item)\n",
    "        try:\n",
    "            hedge_img = Image.open(hedge_path)\n",
    "            # generate hedge masks for every 5%\n",
    "            for density in np.arange(min_density, max_density, density_step):\n",
    "                logger.info(\"processing density: {}\".format(density))\n",
    "                img = hedge_img.copy()\n",
    "                hedge_mask, result_density = generate_hedge_mask(img, density, noise_area_threshold)\n",
    "                file_name = str(i + processed_count) + \"_\" + str(int(density * 100)) + \".png\"\n",
    "                save_path = os.path.join(save_dir, str(result_density), file_name)\n",
    "                hedge_mask.save(save_path)\n",
    "\n",
    "            os.replace(hedge_path, os.path.join(temp_dir, item))\n",
    "        except(OSError, NameError):\n",
    "            logger.info(\"OSError, Path:\", hedge_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bw4V977qKMtO",
    "tags": []
   },
   "source": [
    "## execute hedge mask generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_L9GdN5KMM_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_hedge_masks:\n",
    "    source_path = os.path.join(root, \"images/sources/hedge_images\")\n",
    "    save_dir = os.path.join(root, \"images/sources/hedge_masks\")\n",
    "    temp_dir = os.path.join(root, \"images/sources/hedge_images(processed)\")\n",
    "    noise_area_threshold = 800\n",
    "    min_density=0.1\n",
    "    max_density=0.81\n",
    "    density_step=0.05\n",
    "    processed_count=0\n",
    "    \n",
    "    for density in np.arange(0.1, 0.91, 0.1):\n",
    "        current_path = os.path.join(save_dir, str(round(density, 1)))\n",
    "        os.makedirs(current_path, exist_ok=True)\n",
    "\n",
    "    generate_all_hedge_masks(source_path, save_dir, temp_dir, min_density, max_density, density_step, noise_area_threshold, processed_count)\n",
    "    \n",
    "    drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai-SE0FoGCvb"
   },
   "source": [
    "## generate hedged images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jh2-5LTR-Ko-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_single_hedged_image(input_img, hedge_mask, img_size=(128, 128)):\n",
    "    crop_width = crop_height = 1000\n",
    "    current_background = input_img.copy()\n",
    "\n",
    "    hedge_width, hedge_height = hedge_mask.size\n",
    "    if min(hedge_width, hedge_height) < max(crop_width, crop_height):\n",
    "        crop_width = crop_height = min(hedge_width, hedge_height)\n",
    "\n",
    "    current_background = current_background.resize((crop_width, crop_height))\n",
    "\n",
    "    # find a random point to crop\n",
    "    left = np.random.randint(0, hedge_width - crop_width + 1)\n",
    "    top = np.random.randint(0, hedge_height - crop_height + 1)\n",
    "\n",
    "    current_hedge_mask = hedge_mask.crop((left, top, left + crop_width, top + crop_height))\n",
    "    current_background.paste(current_hedge_mask, (0, 0), mask=current_hedge_mask)\n",
    "    current_background = current_background.resize(img_size)\n",
    "    current_hedge_mask = current_hedge_mask.resize(img_size)\n",
    "    return current_background, current_hedge_mask      \n",
    "\n",
    "\n",
    "def generate_all_hedged_images(hedge_mask_dir, background_paths, target_dir, img_size=(128, 128), sample_no_per_image=10, prefix=0):\n",
    "    \"\"\"\n",
    "    1. store the ground truth, generated hedged images into target path\n",
    "    2. record the corresponding hedge masks used for each hedged images\n",
    "        ground truth -> (target_dir/img_code/img_code.png) done\n",
    "        hedged image -> (target_dir/img_code/0.1/0.png) \n",
    "        hedge mask record -> (target_dir/img_code/0.1/0.json)\n",
    "        \n",
    "    :param hedge_mask_dir: directory containing hedge masks directories grouped with their densities\n",
    "    :param background_paths: a list containing all the full imagenet paths\n",
    "    :param target_dir: directory that would be used to store the generated hedged images (target_dir/img_code/0.1/0.png)\n",
    "    :param img_size: decides the size of the generated hedged image\n",
    "    :param sample_no_per_image: number of hedged images generated for each background\n",
    "    :param prefix: used when adding extra hedge_images to existed datasets to prevent overwritting data\n",
    "    \"\"\"\n",
    "    # iterate through the 2D background paths\n",
    "    for background_in_each_class in background_paths:\n",
    "        dest_label_path = os.path.join(target_dir, background_in_each_class[0].split(\"/\")[-2])\n",
    "        make_dir(dest_label_path)\n",
    "\n",
    "        for full_path in background_in_each_class:\n",
    "            img_code, class_name = full_path.split(\"/\")[-1].split(\".\")[0], full_path.split(\"/\")[-2]\n",
    "            logger.info(\"processing img name: {} with class label: {}\".format(img_code, class_name))\n",
    "\n",
    "\n",
    "            # create directory (target_dir/class_name/img_code)\n",
    "            dest_img_code_path = os.path.join(dest_label_path, img_code)\n",
    "            make_dir(dest_img_code_path)\n",
    "\n",
    "            # read background image\n",
    "            background = Image.open(full_path)\n",
    "            \n",
    "            # save the ground_truth (target_dir/img_code/gt.png)\n",
    "            ground_truth = background.copy()\n",
    "            ground_truth = ground_truth.resize(img_size)\n",
    "            ground_truth.convert('RGB').save(os.path.join(dest_img_code_path, img_code+\".png\"))\n",
    "            \n",
    "            for hedge_density in os.listdir(hedge_mask_dir):\n",
    "                # create directory (target_dir/img_code/0.1)\n",
    "                hedge_masks_path = os.path.join(hedge_mask_dir, hedge_density)\n",
    "                hedge_masks = os.listdir(hedge_masks_path)\n",
    "                dest_hedge_density_path = os.path.join(dest_img_code_path, hedge_density)\n",
    "                make_dir(dest_hedge_density_path)\n",
    "\n",
    "                # repeat for sample_no_per_image times\n",
    "                for i in range(sample_no_per_image):\n",
    "                    img_name = os.path.join(dest_hedge_density_path, str(i+prefix)+\".png\")\n",
    "                    hedge_name = os.path.join(dest_hedge_density_path, str(i+prefix)+\"_hedge.png\")\n",
    "                    \n",
    "                    # if img already existed, just skip it\n",
    "                    if img_name in os.listdir(dest_hedge_density_path) and hedge_name in os.listdir(dest_hedge_density_path):\n",
    "                        continue\n",
    "\n",
    "                    # randomly choose a hedge mask from current hedge density\n",
    "                    hedge_mask_path = np.random.choice(hedge_masks, size=1)[0]\n",
    "                    hedge_mask_path = os.path.join(hedge_masks_path, hedge_mask_path)\n",
    "                    hedge_mask_name = hedge_mask_path.split(\".\")[0]                    \n",
    "                    hedge_mask = Image.open(hedge_mask_path)\n",
    "                    # generate hedged image and hedge mask then save them\n",
    "                    hedged_image, hedge_mask = generate_single_hedged_image(background, hedge_mask, img_size)\n",
    "                    hedged_image.convert('RGB').save(img_name)\n",
    "                    hedge_mask.convert('RGB').save(hedge_name)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIfuP8sBJQIy"
   },
   "source": [
    "## execute hedged image generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbHHJCRkqMhY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 40gb for 1 image in [train/val/test]\n",
    "\n",
    "if generate_hedge_image:\n",
    "    target_root_dir = os.path.join(root, \"images/hedged_images\")\n",
    "    make_dir(target_root_dir)\n",
    "    train_background_paths = os.path.join(root, \"images/sources/imagenet/total_train_paths.json\")\n",
    "    val_background_paths = os.path.join(root, \"images/sources/imagenet/total_val_paths.json\")\n",
    "    test_background_paths = os.path.join(root, \"images/sources/imagenet/total_test_paths.json\")\n",
    "\n",
    "    train_background_paths = load_json(train_background_paths)\n",
    "    val_background_paths = load_json(val_background_paths)\n",
    "    test_background_paths = load_json(test_background_paths)\n",
    "    \n",
    "    train_background_paths.sort()\n",
    "    val_background_paths.sort()\n",
    "    test_background_paths.sort()    \n",
    "    \n",
    "    hedge_dir = os.path.join(root, \"images/sources/hedge_masks\")\n",
    "    data_types = [[\"train\", train_background_paths], [\"val\", val_background_paths], [\"test\", test_background_paths]]\n",
    "    # data_types = [[\"train\", train_background_paths]]\n",
    "    # data_types = [[\"val\", val_background_paths]]\n",
    "    # data_types = [[\"test\", test_background_paths]]\n",
    "    img_size = (128, 128)\n",
    "    sample_no_per_image = 2\n",
    "    prefix = 4\n",
    "\n",
    "    for data_type, background_paths in data_types:\n",
    "        target_dir = os.path.join(target_root_dir, data_type)\n",
    "        make_dir(target_dir)\n",
    "\n",
    "        # split the background_paths into #cpu parts        \n",
    "        path_batches = split_data_into_batches(background_paths, cpus)\n",
    "\n",
    "        \n",
    "        if multi_processing:\n",
    "            pool = Pool(cpus)\n",
    "            print(\"multiprocessing will be run with {} threads\".format(cpus))\n",
    "            # generate_all_hedged_images(hedge_dir, background_paths, target_dir)\n",
    "            pool.starmap(generate_all_hedged_images, [(hedge_dir, background_path, target_dir, img_size, sample_no_per_image, prefix) for background_path in path_batches])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_CZqFsU3mOd"
   },
   "source": [
    "## Validate synthetic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQ1nidhwm8Y5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hedged_images_validation(path, label_count, img_count, hedge_density_count, sample_count):\n",
    "    # 1000 class in total\n",
    "    assert len(os.listdir(path)) == label_count\n",
    "\n",
    "    for class_label in os.listdir(path):\n",
    "        class_path = os.path.join(path, class_label)\n",
    "        # train:32, val=8, test=10\n",
    "        assert len(os.listdir(class_path)) == img_count\n",
    "        \n",
    "        for img_label in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_label)\n",
    "            content_in_density_dir = os.listdir(img_path)\n",
    "            # 8 density + 1 ground truth\n",
    "            assert len(content_in_density_dir) == hedge_density_count\n",
    "            for i in range(1, 9):\n",
    "                assert str(round(i/10, 1)) in content_in_density_dir\n",
    "            assert img_label+\".png\" in content_in_density_dir\n",
    "\n",
    "            for hedge_label in content_in_density_dir:\n",
    "                if \"png\" not in hedge_label:\n",
    "                    try:\n",
    "                        assert len(os.listdir(os.path.join(img_path, hedge_label))) == sample_count\n",
    "                    except:\n",
    "                        print(img_path, hedge_label)\n",
    "                        print(len(os.listdir(os.path.join(img_path, hedge_label))), sample_count)\n",
    "                        \n",
    "if validate_dataset:\n",
    "    label_count = 1000\n",
    "    hedge_density_count = 8+1\n",
    "    sample_count = 6\n",
    "\n",
    "    train_path = os.path.join(root, \"images/hedged_images/train\")\n",
    "    val_path = os.path.join(root, \"images/hedged_images/val\")\n",
    "    test_path = os.path.join(root, \"images/hedged_images/test\")\n",
    "\n",
    "    hedged_images_validation(train_path, 1000, 32, hedge_density_count, sample_count*2)\n",
    "    hedged_images_validation(val_path, 1000, 8, hedge_density_count, sample_count*2)\n",
    "    hedged_images_validation(test_path, 1000, 10, hedge_density_count, sample_count*2)\n",
    "\n",
    "\n",
    "    assert len(os.listdir(train_path)) == len(os.listdir(val_path)) == len(os.listdir(test_path)) == 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRHTo4VcBcCR"
   },
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2T0f34-KBdPJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def vgg_accuracy(model, images, labels):\n",
    "    # there is 64 labels and images coming in\n",
    "    model.eval()\n",
    "    pred = model(images)\n",
    "    label = labels.reshape(labels.shape[0], 1)\n",
    "\n",
    "    # top-1\n",
    "    _, predicted = torch.topk(pred.data, k=1, dim=1)\n",
    "    top_1_result = torch.sum(predicted == label).item()\n",
    "\n",
    "    # top-5\n",
    "    _, predicted = torch.topk(pred.data, k=5, dim=1)\n",
    "    top_5_result = np.sum([label[i] in predicted[i] for i in range(labels.shape[0])])\n",
    "\n",
    "    size = labels.shape[0]\n",
    "    return top_1_result / size, top_5_result / size\n",
    "\n",
    "\n",
    "def uncertainty_MSE_loss(pred, uncertainty, target):\n",
    "    mse = F.mse_loss(pred, target, reduction=\"none\")\n",
    "    loss = torch.mean(torch.exp(-uncertainty) * mse + uncertainty)\n",
    "    return loss, mse.detach().mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zPAklA-BASW"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBre14mpBzZY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUWKZeUhEYwt"
   },
   "source": [
    "## Static Hedge Dataset\n",
    "Used to load pre-generated hedged images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvNHe1J9BCik",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StaticHedgedDataset(Dataset):\n",
    "    def __init__(self, root_dir, density_range, masked_img_per_item, require_hedge_mask=False, color_mode=\"RGB\", debug=False):\n",
    "        # read args\n",
    "        self.root_dir = root_dir\n",
    "        self.density_range = density_range\n",
    "        self.masked_img_per_item = masked_img_per_item\n",
    "        self.require_hedge_mask = require_hedge_mask\n",
    "        \n",
    "        # collect dataset size\n",
    "        self.label_count = -1\n",
    "        self.image_count = -1\n",
    "        self.density_count = -1\n",
    "        self.sample_count = -1\n",
    "        self.available_samples = None\n",
    "        self.get_image_counts()\n",
    "\n",
    "        # read image paths\n",
    "        self.image_path = []\n",
    "        self.read_image_code_path()\n",
    "        \n",
    "        # validation on dataset\n",
    "        self.image_code_path_validation()\n",
    "        self.to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "        self.debug = debug\n",
    "        self.color_mode = color_mode\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        if self.debug:\n",
    "            return 100\n",
    "        else:\n",
    "            return self.label_count * self.image_count\n",
    "\n",
    "\n",
    "    def read_img(self, path):\n",
    "        im = np.array(Image.open(path).convert(\"RGB\"))\n",
    "        if self.color_mode == \"RGB\":\n",
    "            return self.to_tensor(im)\n",
    "        elif self.color_mode == \"HSV\":\n",
    "            return self.to_tensor(cv2.cvtColor(im, cv2.COLOR_RGB2HSV))\n",
    "        elif self.color_mode == \"LAB\":\n",
    "            return self.to_tensor(cv2.cvtColor(im, cv2.COLOR_RGB2LAB))\n",
    "        elif self.color_mode == \"LUV\":\n",
    "            return self.to_tensor(cv2.cvtColor(im, cv2.COLOR_RGB2Luv))\n",
    "        else:\n",
    "            logger.info(\"color mode not supported\")\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return #masked_img_per_item masked image, also returns hedge masks if required\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # choose a random label + img_code\n",
    "            current_image_path = self.image_path[random.randint(0, self.label_count - 1)][random.randint(0, self.image_count - 1)]\n",
    "            label = int(current_image_path.split(\"/\")[-2])\n",
    "            density = str(round(self.density_range[idx % len(self.density_range)], 2))  # uniform density across dataset\n",
    "            \n",
    "            # get the ground truth\n",
    "            ground_truth_path = os.path.join(current_image_path, current_image_path.split(\"/\")[-1] + \".png\")\n",
    "            ground_truth_image = self.read_img(ground_truth_path)\n",
    "\n",
    "            # choose n samples from same density level and load all of them\n",
    "            samples = np.random.choice(self.available_samples, self.masked_img_per_item, replace=False)\n",
    "            masked_images = []\n",
    "            hedge_masks = []\n",
    "            for sample in samples:\n",
    "                masked_image_path = os.path.join(current_image_path, density, str(int(sample)) + \".png\")\n",
    "                masked_images.append(self.read_img(masked_image_path))\n",
    "                \n",
    "            # return hedge masks if required\n",
    "            if self.require_hedge_mask:\n",
    "                for sample in samples:\n",
    "                    full_hedge_path = os.path.join(current_image_path, density, sample + \"_hedge.png\")\n",
    "                    hedge_masks.append(self.read_img(full_hedge_path))\n",
    "\n",
    "            return masked_images, ground_truth_image, hedge_masks, label\n",
    "\n",
    "        # if can't find try another one just in case if any file is missing\n",
    "        except (FileNotFoundError, UnidentifiedImageError) as e:\n",
    "            logger.info(\"{} not found / is broken\".format(masked_image_path))\n",
    "            return self.__getitem__(idx)\n",
    "\n",
    "\n",
    "    def get_image_counts(self):\n",
    "        self.label_count = len(os.listdir(self.root_dir))\n",
    "        label_path = os.path.join(self.root_dir, os.listdir(self.root_dir)[0])\n",
    "        \n",
    "        self.image_count = len(os.listdir(label_path))\n",
    "        image_path = os.path.join(label_path, os.listdir(label_path)[0])\n",
    "        \n",
    "        self.density_count = len(self.density_range)\n",
    "        density_path = os.path.join(image_path, os.listdir(image_path)[0])\n",
    "        \n",
    "        self.sample_count = int(len(os.listdir(density_path)) / 2)  # hedged background and hedge mask so div by 2\n",
    "        if self.masked_img_per_item > self.sample_count:\n",
    "            print(\"warning: sample per density isn't enough\")\n",
    "\n",
    "        self.available_samples = np.arange(0, self.sample_count)\n",
    "        \n",
    "\n",
    "    def read_image_code_path(self):\n",
    "        \"\"\"\n",
    "        We store the path for each image in a multi-dimension list with [label, image_code]\n",
    "        Density/Sample would be decided during get_item()\n",
    "        \"\"\"\n",
    "        for label_no in os.listdir(self.root_dir):\n",
    "            current_label = []\n",
    "            label_dir = os.path.join(self.root_dir, label_no)\n",
    "            for image_code in os.listdir(label_dir):\n",
    "                current_label.append(os.path.join(label_dir, image_code))\n",
    "            self.image_path.append(current_label)\n",
    "        self.image_path = np.array(self.image_path)\n",
    "\n",
    "\n",
    "    def image_code_path_validation(self):\n",
    "        \"\"\" make sure all files are loaded correctly \"\"\"\n",
    "        assert self.image_path.shape == (self.label_count, self.image_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nLfUyel_PQj"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSv1hKEc_SPs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7vnCNJ2_23r"
   },
   "source": [
    "## U-Net\n",
    "Used to perform image dehedging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX4aPEeSD7ts"
   },
   "source": [
    "### U-Net Block (TODO try PReLU + Dropout2d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fL1f6H7LW1us"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "#         self.block = nn.Sequential(nn.Conv2d(in_channel, out_channel, 3, padding=1, padding_mode='reflect'),\n",
    "#                                    nn.LeakyReLU(),\n",
    "#                                    nn.BatchNorm2d(out_channel, affine=True),\n",
    "#                                    nn.Conv2d(out_channel, out_channel, 3, padding=1, padding_mode='reflect'),\n",
    "#                                    nn.LeakyReLU(),\n",
    "#                                    nn.BatchNorm2d(out_channel, affine=True))\n",
    "\n",
    "        # 1) replace LeakyReLU with ParametricReLU (discard due to memory issue)\n",
    "        # 2) added dropout with p=0.01\n",
    "        self.block = nn.Sequential(nn.Conv2d(in_channel, out_channel, 3, padding=1, padding_mode='replicate'),\n",
    "                                   nn.LeakyReLU(),\n",
    "                                   # nn.Dropout2d(p=0.01),\n",
    "                                   nn.BatchNorm2d(out_channel, affine=True),\n",
    "                                   nn.Conv2d(out_channel, out_channel, 3, padding=1, padding_mode='replicate'),\n",
    "                                   nn.LeakyReLU(),\n",
    "                                   # nn.Dropout2d(p=0.01),\n",
    "                                   nn.BatchNorm2d(out_channel, affine=True))\n",
    "\n",
    "        self.block.apply(self.weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQeZchOQALYR"
   },
   "source": [
    "### U-Net Encoder/Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onR6GwYkANUw"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(in_channels[0], in_channels[1])])\n",
    "        self.down_samples = nn.ModuleList([])\n",
    "\n",
    "        for i in range(1, len(in_channels) - 1):\n",
    "            self.blocks.append(Block(in_channels[i + 1], in_channels[i + 1]))\n",
    "            self.down_samples.append(\n",
    "                nn.Conv2d(in_channels[i], in_channels[i + 1], 3, stride=2, padding=1, padding_mode='reflect'))\n",
    "\n",
    "    def forward(self, x):\n",
    "        connections = [x]\n",
    "        for i in range(len(self.down_samples)):\n",
    "            x = self.blocks[i](x)\n",
    "            connections.append(x)\n",
    "            x = self.down_samples[i](x)\n",
    "        x = self.blocks[-1](x)\n",
    "        return x, connections\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        self.up_samples = nn.ModuleList([])\n",
    "\n",
    "        for i in range(0, len(out_channels) - 2):\n",
    "            self.up_samples.append(\n",
    "                nn.ConvTranspose2d(out_channels[i], out_channels[i + 1], 3, stride=2, padding=1, output_padding=1))\n",
    "            self.blocks.append(Block(out_channels[i], out_channels[i + 1]))\n",
    "\n",
    "        self.blocks.append(Block(out_channels[-2], out_channels[-1]))\n",
    "\n",
    "    def forward(self, x, connections):\n",
    "        for i in range(len(self.up_samples)):\n",
    "            x = self.up_samples[i](x)\n",
    "            x = torch.cat([x, connections[i]], dim=1)\n",
    "            x = self.blocks[i](x)\n",
    "        x = self.blocks[-1](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEDawdlsAO9f"
   },
   "source": [
    "### U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOSzgY1aAR5g"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        # in_channels = [c_in, 32, 64, 128, 256]\n",
    "        # out_channels = [256, 128, 64, 32, c_out]\n",
    "        in_channels = [c_in, 64, 128, 256, 512]\n",
    "        out_channels = [512, 256, 128, 64, c_out]\n",
    "        \n",
    "        \n",
    "        self.encoder = Encoder(in_channels)\n",
    "        self.decoder = Decoder(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, connections = self.encoder(x)\n",
    "        output_pred = self.decoder(x, connections[::-1])\n",
    "        return output_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEnLW0U3NUGG"
   },
   "source": [
    "### Uncertainty U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN5JGiAvNWOP"
   },
   "outputs": [],
   "source": [
    "class UncertaintyUNet(nn.Module):\n",
    "    def __init__(self, c_in, c_out, u_out):\n",
    "        super().__init__()\n",
    "        # in_channels = [c_in, 32, 64, 128, 256]\n",
    "        # out_channels = [256, 128, 64, 32, c_out]\n",
    "        # uncertainty_channels = [256, 128, 64, 32, u_out]\n",
    "\n",
    "        in_channels = [c_in, 64, 128, 256, 512]\n",
    "        out_channels = [512, 256, 128, 64, c_out]\n",
    "        uncertainty_channels = [512, 256, 128, 64, c_out]\n",
    "        \n",
    "        self.encoder = Encoder(in_channels)\n",
    "        self.decoder = Decoder(out_channels)\n",
    "        self.uncertainty_decoder = Decoder(uncertainty_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, connections = self.encoder(x)\n",
    "        output_pred = self.decoder(x, connections[::-1])\n",
    "        uncertainty_pred = self.uncertainty_decoder(x, connections[::-1])\n",
    "        # TODO test on torch.clamp on prediction to force net output stays in range [0, 1]\n",
    "        output_pred = torch.clamp(output_pred, 0, 1)\n",
    "        return output_pred, uncertainty_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQsSLJMaAfZR"
   },
   "source": [
    "## VGG Model\n",
    "Used to perform sanity check on model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFrjyluPAkrn"
   },
   "outputs": [],
   "source": [
    "class VGGClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGClass, self).__init__()\n",
    "        self.vgg_net = torchvision.models.vgg16(pretrained=True)\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        input_img = torch.clamp(input_img, 0, 1)\n",
    "        input_img = self.preprocess(input_img)\n",
    "        vgg_class = self.vgg_net(input_img)\n",
    "        return vgg_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwAx2ruHNozN"
   },
   "source": [
    "### Validate VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcBG_zSwPQFF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from dataset import StaticHedgedDataset\n",
    "# from model import VGGClass\n",
    "# from config import device\n",
    "# from utils import AverageMeter\n",
    "# from loss_functions import vgg_accuracy\n",
    "\n",
    "\n",
    "class ImageNetDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        # read args\n",
    "        self.root_dir = root_dir\n",
    "        self.image_path = []\n",
    "        self.read_image_code_path()\n",
    "        self.to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "\n",
    "    def read_img(self, path):\n",
    "        return self.to_tensor(cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB))\n",
    "        # return self.to_tensor(cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)).resize(224, 224, 3)\n",
    "        # return self.to_tensor(Image.open(path).convert(\"RGB\").resize(224, 224, 3))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return #masked_img_per_item masked image, also returns hedge masks if required\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # choose a random label + img_code\n",
    "            current_image_path = self.image_path[idx]\n",
    "            label = int(current_image_path.split(\"/\")[-2])\n",
    "            image = self.read_img(current_image_path)\n",
    "            return image, label\n",
    "\n",
    "        # if can't find try another one just in case if any file is missing\n",
    "        except FileNotFoundError:\n",
    "            print(\"{} not found\".format(masked_image_path))\n",
    "            return self.__getitem__(idx)\n",
    "        \n",
    "\n",
    "    def read_image_code_path(self):\n",
    "        \"\"\"\n",
    "        We store the path for each image in a multi-dimension list with [label, image_code]\n",
    "        Density/Sample would be decided during get_item()\n",
    "        \"\"\"\n",
    "        for label_no in os.listdir(self.root_dir):\n",
    "            label_dir = os.path.join(self.root_dir, label_no)\n",
    "            for image_code in os.listdir(label_dir):\n",
    "                self.image_path.append(os.path.join(label_dir, image_code))\n",
    "        self.image_path = np.array(self.image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## VGG on (224,224) imageNet dataset (original version)\n",
    "# vgg_model = VGGClass().to(device)\n",
    "# vgg_model.eval()\n",
    "\n",
    "# path = os.path.join(root, \"images/sources/imagenet/data_VGG_label\")\n",
    "# dataset = ImageNetDataset(path)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, num_workers=0)\n",
    "# top1_acc_meter = AverageMeter()\n",
    "# top5_acc_meter = AverageMeter()\n",
    "\n",
    "# # top 1 should be about 0.625, top 5 should be around 0.85\n",
    "# for i, (image, label) in enumerate(dataloader):\n",
    "#     image = image.to(device)\n",
    "#     label = label.to(device)\n",
    "\n",
    "#     top1_acc, top5_acc = vgg_accuracy(vgg_model, image, label)\n",
    "\n",
    "#     update_meters([top1_acc_meter, top5_acc_meter], [top1_acc, top5_acc])\n",
    "\n",
    "#     if i % 500 == 0:\n",
    "#         logger.info(\"{}/{} top 1 {}, top 5 {}\".format(i, len(dataloader), top1_acc_meter.avg, top5_acc_meter.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sx8NVJ3dNqRI"
   },
   "outputs": [],
   "source": [
    "# ## VGG on (128,128) imageNet dataset (our version)\n",
    "# path = os.path.join(root, \"images/hedged_images/test\")\n",
    "# density_range = np.arange(0.1, 0.9, 0.1)\n",
    "# dataset = StaticHedgedDataset(path, density_range, masked_img_per_item=1, require_hedge_mask=False)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "# top1_acc_meter.reset()\n",
    "# top5_acc_meter.reset()\n",
    "\n",
    "# # top 1 should be about 0.625, top 5 should be around 0.85\n",
    "# for i, (_, ground_truths, _,  label) in enumerate(dataloader):\n",
    "#     ground_truths = ground_truths.to(device)\n",
    "#     label = label.to(device)\n",
    "\n",
    "#     top1_acc, top5_acc = vgg_accuracy(vgg_model, ground_truths, label)\n",
    "    \n",
    "#     update_meters([top1_acc_meter, top5_acc_meter], [top1_acc, top5_acc])\n",
    "    \n",
    "#     if i % 50 == 0:\n",
    "#         logger.info(\"{}/{} top 1 {}, top 5 {}\".format(i, len(dataloader), top1_acc_meter.avg, top5_acc_meter.avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEh6cUAOBpqt"
   },
   "source": [
    "# Optimizer Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSACbLlXBpW3"
   },
   "outputs": [],
   "source": [
    "class OptimizerWrapper(object):\n",
    "\n",
    "    def __init__(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # self.step_num = 0\n",
    "        # self.lr = 0.1\n",
    "\n",
    "    def clip_gradient(self, clip_val):\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.clamp_(-clip_val, clip_val)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        # self._update_lr()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def adjust_lr(self, lr):\n",
    "        for param in self.optimizer.param_groups:\n",
    "            param['lr'] = lr\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcwTjdU5envX"
   },
   "source": [
    "# Train U-Net\n",
    "\n",
    "reason for adding scheduler for adam:\n",
    "https://arxiv.org/abs/1711.05101\n",
    "\n",
    "\"Adam can substantially benefit from a scheduled learning rate multiplier. The fact that Adam\n",
    "is an adaptive gradient algorithm and as such adapts the learning rate for each parameter\n",
    "does not rule out the possibility to substantially improve its performance by using a global\n",
    "learning rate multiplier, scheduled, e.g., by cosine annealing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import madgrad\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau, OneCycleLR\n",
    "# from done.utils import AverageMeter, save_checkpoint, get_logger, show_tensor_images\n",
    "# from done.VGGClass import VGGClass\n",
    "# from done.model import UNet\n",
    "# from done.dataset import StaticHedgedDataset\n",
    "# from done.loss_functions import vgg_accuracy\n",
    "# from done.config import device, print_freq, parse_args\n",
    "# from done.optimizer_wrapper import OptimizerWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_mse_param(uncertainty_unet, path_to_pretrained_mse_unet, pretrained_fixed=True):\n",
    "    checkpoint = torch.load(path_to_pretrained_mse_unet)\n",
    "    pretrained_model = checkpoint[\"u_net_model\"]\n",
    "    if isinstance(pretrained_model, torch.nn.DataParallel):\n",
    "        pretrained_model = pretrained_model.module\n",
    "    uncertainty_unet.encoder.load_state_dict(pretrained_model.encoder.state_dict())\n",
    "    uncertainty_unet.decoder.load_state_dict(pretrained_model.decoder.state_dict())\n",
    "    \n",
    "    if pretrained_fixed:\n",
    "        for param in uncertainty_unet.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in uncertainty_unet.decoder.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    del pretrained_model\n",
    "    \n",
    "    \n",
    "def log_and_record(logger, writer, train_loss, val_loss, ep, train_mse=None, train_uncertainty=None, val_mse=None, val_uncertainty=None):\n",
    "\n",
    "    if args.unet_type == \"unet\":\n",
    "        logger.info('Epoch: [{0}]\\t'\n",
    "                    'Train Loss {train_loss:.5f}\\t'\n",
    "                    'Val Loss {val_loss:.5f}\\t'.format(ep, train_loss=train_loss, val_loss=val_loss))\n",
    "\n",
    "        writer.add_scalar('model/train_loss', train_loss, ep)\n",
    "        writer.add_scalar('model/val_loss', val_loss, ep)\n",
    "\n",
    "    elif args.unet_type == \"uncertainty_unet\" or args.unet_type == \"recurrent_uncertainty_unet\":\n",
    "        logger.info('\\n\\nEpoch: [{0}]\\n'\n",
    "                    'Train Loss {train_loss:.5f}\\t'\n",
    "                    'Train MSE {train_mse:.5f}\\t'\n",
    "                    'Train average uncertainty value {train_uncertainty:.5f}\\n'\n",
    "                    'Val Loss {val_loss:.5f}\\t'\n",
    "                    'Val MSE {val_mse:.5f}\\t\\t'\n",
    "                    'Val average uncertainty {val_uncertainty:.5f}\\n'.format(ep,\n",
    "                                                                             train_loss=train_loss,\n",
    "                                                                             train_mse=train_mse,\n",
    "                                                                             train_uncertainty=train_uncertainty,\n",
    "                                                                             val_loss=val_loss,\n",
    "                                                                             val_mse=val_mse,\n",
    "                                                                             val_uncertainty=val_uncertainty))\n",
    "        \n",
    "        writer.add_scalar('model/train_loss', train_loss, ep)\n",
    "        writer.add_scalar('model/uncertainty', train_uncertainty, ep)\n",
    "        writer.add_scalar('model/mse', train_mse, ep)\n",
    "        writer.add_scalar('model/val_loss', val_loss, ep)\n",
    "        writer.add_scalar('model/uncertainty', val_uncertainty, ep)\n",
    "        writer.add_scalar('model/mse', val_mse, ep)\n",
    "\n",
    "        \n",
    "def get_scheduler(optimizer, steps_per_epoch, start_epoch):\n",
    "    if args.scheduler == 'exp':\n",
    "        return ExponentialLR(optimizer.optimizer, gamma=args.LR_gamma, verbose=args.verbose)\n",
    "    elif args.scheduler == 'cosine':\n",
    "        return CosineAnnealingWarmRestarts(optimizer.optimizer, T_0=args.T_0, T_mult=args.T_mult, eta_min=args.eta_min, verbose=args.verbose)\n",
    "    elif args.scheduler == 'ReduceLROnPlateau':\n",
    "        return ReduceLROnPlateau(optimizer.optimizer, mode=args.mode, factor=args.factor, patience=args.patience, min_lr=args.min_lr, verbose=args.verbose)\n",
    "    elif args.scheduler == 'onecycle':\n",
    "        return OneCycleLR(optimizer.optimizer, max_lr=args.max_lr, steps_per_epoch=steps_per_epoch, epochs=args.end_epoch - start_epoch)\n",
    "    else:\n",
    "        raise TypeError('scheduler {} is not supported.'.format(args.scheduler))\n",
    "        logger.info(\"using {} as scheduler\".format(args.scheduler))\n",
    "        \n",
    "def scheduler_step(scheduler, val_loss):\n",
    "    if args.scheduler=='ReduceLROnPlateau':\n",
    "        scheduler.step(val_loss)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "        \n",
    "    logger.info(\"learning rate: {}\".format(scheduler.optimizer.param_groups[0]['lr']))    \n",
    "    \n",
    "    \n",
    "def get_data_parallel_model(model):\n",
    "    # if there are multiple GPU available and the model is not a DataParallel Object\n",
    "    if torch.cuda.device_count() > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "        model = nn.DataParallel(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_training(start_epoch, end_epoch, train_loader, val_loader, model, criterion, optimizer, image_save_path, logger, writer, scheduler, best_loss, epochs_since_improvement):\n",
    "    for ep in range(start_epoch, end_epoch):\n",
    "        train_loss = train_unet(train_loader, model, criterion, optimizer, args.clip_val, ep, scheduler)          \n",
    "        val_loss = val_unet(val_loader, model, criterion, ep, image_save_path)\n",
    "        log_and_record(logger, writer, train_loss, val_loss, ep)\n",
    "                               \n",
    "        if args.scheduler and args.scheduler != \"onecycle\":\n",
    "            scheduler_step(scheduler, val_loss)\n",
    "\n",
    "        # due to the uncertainty property there are chance that the loss go up to inf or nan\n",
    "        # so we have to treat them differently\n",
    "        if math.isnan(val_loss) or math.isinf(val_loss):\n",
    "            is_best = False\n",
    "        else:\n",
    "            is_best = val_loss < best_loss\n",
    "            best_loss = min(val_loss, best_loss)\n",
    "            \n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            logger.info(\"Epochs since last improvement: {}\".format(epochs_since_improvement))\n",
    "            if epochs_since_improvement > args.early_stop_ep:\n",
    "                logger.info(\"early stop at ep {} with no improvement for {} eps\".format(ep, args.early_stop_ep))\n",
    "                return      \n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "        save_checkpoint(ep, epochs_since_improvement, model, optimizer, scheduler, val_loss, best_loss, is_best, args.checkpoint_save_path)\n",
    "\n",
    "                            \n",
    "\n",
    "        \n",
    "def train_unet(train_loader, model, criterion, optimizer, clip_val, epoch, scheduler=None):\n",
    "    # train mode\n",
    "    model.train()\n",
    "\n",
    "    if args.scheduler == \"cosine\":\n",
    "        iters = len(train_loader)\n",
    "    # train\n",
    "    training_loss = AverageMeter()\n",
    "\n",
    "    for i, (masked_images, ground_truths, _,  _) in enumerate(train_loader):\n",
    "        ground_truths = ground_truths.to(device)\n",
    "        masked_images[0] = masked_images[0].to(device)\n",
    "\n",
    "        # L2 loss training\n",
    "        dehedged_prediction = model(masked_images[0])\n",
    "        loss = criterion(dehedged_prediction, ground_truths)\n",
    "\n",
    "        # back prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        if clip_val:\n",
    "            optimizer.clip_gradient(clip_val)\n",
    "\n",
    "        optimizer.step()\n",
    "        training_loss.update(loss.item())\n",
    "\n",
    "        # onecyle scheduler step for every batches\n",
    "        if args.scheduler == \"onecycle\":\n",
    "            scheduler.step()\n",
    "        elif args.scheduler == \"cosine\":\n",
    "            scheduler.step(ep+i/iters)\n",
    "            \n",
    "        # if i % print_freq == 0:\n",
    "        #     logger.info('Epoch: [{0}][{1}/{2}]\\t'\n",
    "        #                 'Train Loss {loss.val:.5f} ({loss.avg:.5f})\\t'.format(epoch, i, len(train_loader), loss=training_loss))\n",
    "\n",
    "    return training_loss.avg\n",
    "\n",
    "\n",
    "def val_unet(val_loader, model, criterion, epoch, image_save_path):\n",
    "    with torch.no_grad():\n",
    "        # eval mode\n",
    "        model.eval()\n",
    "        # loss values init\n",
    "        validation_loss = AverageMeter()\n",
    "        for i, (masked_images, ground_truths, _, label) in enumerate(val_loader):\n",
    "            ground_truths = ground_truths.to(device)\n",
    "            masked_images[0] = masked_images[0].to(device)\n",
    "            label = label.to(device)\n",
    "            # update L2 loss\n",
    "            dehedged_predictions = model(masked_images[0])\n",
    "            loss = criterion(dehedged_predictions, ground_truths)\n",
    "            validation_loss.update(loss.item())\n",
    "\n",
    "            # if i % print_freq == 0:\n",
    "            #     show_tensor_images([masked_images[0][0], dehedged_predictions[0], ground_truths[0]], [\"masked image\", \"pred\", \"ground truth\"])\n",
    "            #     logger.info('Epoch: [{0}][{1}/{2}]\\t'\n",
    "            #                 'Val Loss {loss.val:.5f} ({loss.avg:.5f})\\t'.format(epoch, i, len(val_loader), loss=validation_loss))\n",
    "\n",
    "        # log.txt info\n",
    "        show_tensor_images([masked_images[0][0], dehedged_predictions[0], ground_truths[0]], [\"masked image\", \"pred\", \"ground truth\"], os.path.join(image_save_path, str(epoch) + \".png\"))\n",
    "\n",
    "        return validation_loss.avg   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_unet_training(start_epoch, end_epoch, train_loader, val_loader, model, criterion, optimizer, image_save_path, logger, writer, scheduler, best_loss, epochs_since_improvement):\n",
    "    for ep in range(start_epoch, end_epoch):\n",
    "        train_loss, train_mse, train_uncertainty = train_uncertainty_unet(train_loader, model, criterion, optimizer, args.clip_val, ep, scheduler)\n",
    "        val_loss, val_mse, val_uncertainty = val_uncertainty_unet(val_loader, model, criterion, ep, image_save_path)\n",
    "        log_and_record(logger, writer, train_loss, val_loss, ep, train_mse, train_uncertainty, val_mse, val_uncertainty)\n",
    "        \n",
    "        if args.scheduler and args.scheduler != \"onecycle\" and args.scheduler != \"cosine\":\n",
    "            scheduler_step(scheduler, val_loss)\n",
    "\n",
    "        # due to the uncertainty property there are chance that the loss go up to inf or nan\n",
    "        # so we have to treat them differently\n",
    "        if math.isnan(val_loss) or math.isinf(val_loss):\n",
    "            is_best = False\n",
    "        else:\n",
    "            is_best = val_loss < best_loss\n",
    "            best_loss = min(val_loss, best_loss)\n",
    "            \n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            logger.info(\"Epochs since last improvement: {}\".format(epochs_since_improvement))\n",
    "            if epochs_since_improvement > args.early_stop_ep:\n",
    "                logger.info(\"early stop at ep {} with no improvement for {} eps\".format(ep, args.early_stop_ep))\n",
    "                return      \n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "                \n",
    "        save_checkpoint(ep, epochs_since_improvement, model, optimizer, scheduler, val_loss, best_loss, is_best, args.checkpoint_save_path)\n",
    "\n",
    "\n",
    "def train_uncertainty_unet(train_loader, model, criterion, optimizer, clip_val, epoch, scheduler=None):\n",
    "    # train mode\n",
    "    model.train()\n",
    "\n",
    "    # loss value init\n",
    "    training_loss = AverageMeter()\n",
    "    mse_meter = AverageMeter()\n",
    "    uncertainty_meter = AverageMeter()\n",
    "\n",
    "    for i, (masked_images, ground_truths, _,  _) in enumerate(train_loader):\n",
    "        ground_truths = ground_truths.to(device)\n",
    "        masked_images[0] = masked_images[0].to(device)\n",
    "\n",
    "        # L2 loss training\n",
    "        dehedged_prediction, uncertainty = model(masked_images[0])\n",
    "        loss, mse = criterion(dehedged_prediction, uncertainty, ground_truths)\n",
    "\n",
    "        # back prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        if clip_val:\n",
    "            optimizer.clip_gradient(clip_val)\n",
    "\n",
    "        optimizer.step()\n",
    "        training_loss.update(loss.item())\n",
    "        mse_meter.update(mse.mean().item())\n",
    "        uncertainty_meter.update(torch.mean(uncertainty))\n",
    "        \n",
    "        # onecyle scheduler step for every batches\n",
    "        if args.scheduler == \"onecycle\" or args.scheduler == \"consine\":\n",
    "            scheduler.step()\n",
    "        # if i % print_freq == 0:\n",
    "        #     logger.info('Epoch: [{0}][{1}/{2}]\\t'\n",
    "        #                 'Train Loss {loss.val:.5f} ({loss.avg:.5f})\\t'.format(epoch, i, len(train_loader), loss=training_loss))\n",
    "\n",
    "    return training_loss.avg, mse_meter.avg, uncertainty_meter.avg\n",
    "\n",
    "\n",
    "def val_uncertainty_unet(val_loader, model, criterion, epoch, image_save_path):\n",
    "    with torch.no_grad():\n",
    "        # eval mode\n",
    "        model.eval()    \n",
    "        # loss values init\n",
    "        validation_loss = AverageMeter()\n",
    "        mse_meter = AverageMeter()\n",
    "        uncertainty_meter = AverageMeter()\n",
    "        for i, (masked_images, ground_truths, _, label) in enumerate(val_loader):\n",
    "            ground_truths = ground_truths.to(device)\n",
    "            masked_images[0] = masked_images[0].to(device)\n",
    "            label = label.to(device)\n",
    "            # update L2 loss\n",
    "            dehedged_predictions, uncertainty = model(masked_images[0])\n",
    "            loss, mse = criterion(dehedged_predictions, uncertainty, ground_truths)\n",
    "\n",
    "            validation_loss.update(loss.item())\n",
    "            mse_meter.update(mse.mean().item())\n",
    "            uncertainty_meter.update(torch.mean(uncertainty))\n",
    "\n",
    "            # if i % print_freq == 0:\n",
    "            #     show_tensor_images([masked_images[0][0], dehedged_predictions[0], ground_truths[0]], [\"masked image\", \"pred\", \"ground truth\"])\n",
    "            #     logger.info('Epoch: [{0}][{1}/{2}]\\t'\n",
    "            #                 'Val Loss {loss.val:.5f} ({loss.avg:.5f})\\t'.format(epoch, i, len(val_loader), loss=validation_loss))\n",
    "\n",
    "        # log.txt info\n",
    "        show_uncertainty_result(masked_images[0][0], dehedged_predictions[0], ground_truths[0], uncertainty[0], os.path.join(image_save_path, str(epoch) + \".png\"))\n",
    "\n",
    "        return validation_loss.avg, mse_meter.avg, uncertainty_meter.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Uncertainty Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_uncertainty_unet_training(start_epoch, end_epoch, train_loader, val_loader, model, single_frame_model, criterion, optimizer, image_save_path, logger, writer, scheduler, best_loss, epochs_since_improvement):\n",
    "    for ep in range(start_epoch, end_epoch):\n",
    "        train_loss, train_mse, train_uncertainty = train_recurrent_uncertainty_unet(train_loader, model, single_frame_model, criterion, optimizer, args.clip_val, ep, scheduler)\n",
    "        val_loss, val_mse, val_uncertainty = val_recurrent_uncertainty_unet(val_loader, model, single_frame_model, criterion, ep, image_save_path)\n",
    "        log_and_record(logger, writer, train_loss, val_loss, ep, train_mse, train_uncertainty, val_mse, val_uncertainty)\n",
    "        \n",
    "        if args.scheduler and args.scheduler != \"onecycle\" and args.scheduler != \"cosine\":\n",
    "            scheduler_step(scheduler, val_loss)\n",
    "\n",
    "        # due to the uncertainty property there are chance that the loss go up to inf or nan\n",
    "        # so we have to treat them differently\n",
    "        if math.isnan(val_loss) or math.isinf(val_loss):\n",
    "            is_best = False\n",
    "        else:\n",
    "            is_best = val_loss < best_loss\n",
    "            best_loss = min(val_loss, best_loss)\n",
    "            \n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            logger.info(\"Epochs since last improvement: {}\".format(epochs_since_improvement))\n",
    "            if epochs_since_improvement > args.early_stop_ep:\n",
    "                logger.info(\"early stop at ep {} with no improvement for {} eps\".format(ep, args.early_stop_ep))\n",
    "                return      \n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "                \n",
    "        save_checkpoint(ep, epochs_since_improvement, model, optimizer, scheduler, val_loss, best_loss, is_best, args.checkpoint_save_path)\n",
    "        \n",
    "        \n",
    "def train_recurrent_uncertainty_unet(train_loader, model, single_frame_model, criterion, optimizer, clip_val, epoch, scheduler=None):\n",
    "    # train mode\n",
    "    model.train()\n",
    "\n",
    "    # loss value init\n",
    "    training_loss = AverageMeter()\n",
    "    mse_meter = AverageMeter()\n",
    "    uncertainty_meter = AverageMeter()\n",
    "\n",
    "    for i, (masked_images, ground_truths, _,  _) in enumerate(train_loader):\n",
    "        ground_truths = ground_truths.to(device)\n",
    "        masked_images[0] = masked_images[0].to(device)\n",
    "        masked_images[1] = masked_images[1].to(device)\n",
    "        \n",
    "        # L2 loss training\n",
    "        with torch.no_grad():\n",
    "            dehedged_predictions, uncertainty = single_frame_model(masked_images[0])\n",
    "            model_input = torch.cat([masked_images[1], dehedged_predictions, uncertainty], dim=1).detach()\n",
    "            model_input = model_input.to(device)\n",
    "            \n",
    "        dehedged_predictions, uncertainty = model(model_input)\n",
    "        loss, mse = criterion(dehedged_predictions, uncertainty, ground_truths)\n",
    "\n",
    "        # back prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        if clip_val:\n",
    "            optimizer.clip_gradient(clip_val)\n",
    "\n",
    "        optimizer.step()\n",
    "        training_loss.update(loss.item())\n",
    "        mse_meter.update(mse.mean().item())\n",
    "        uncertainty_meter.update(torch.mean(uncertainty))\n",
    "        \n",
    "        # onecyle scheduler step for every batches\n",
    "        if args.scheduler == \"onecycle\" or args.scheduler == \"consine\":\n",
    "            scheduler.step()\n",
    "        # if i % print_freq == 0:\n",
    "        #     logger.info('Epoch: [{0}][{1}/{2}]\\t'\n",
    "        #                 'Train Loss {loss.val:.5f} ({loss.avg:.5f})\\t'.format(epoch, i, len(train_loader), loss=training_loss))\n",
    "\n",
    "    return training_loss.avg, mse_meter.avg, uncertainty_meter.avg\n",
    "\n",
    "\n",
    "def val_recurrent_uncertainty_unet(val_loader, model, single_frame_model, criterion, epoch, image_save_path):\n",
    "    with torch.no_grad():\n",
    "        # eval mode\n",
    "        model.eval()    \n",
    "        # loss values init\n",
    "        validation_loss = AverageMeter()\n",
    "        mse_meter = AverageMeter()\n",
    "        uncertainty_meter = AverageMeter()\n",
    "        for i, (masked_images, ground_truths, _, label) in enumerate(val_loader):\n",
    "            ground_truths = ground_truths.to(device)\n",
    "            masked_images[0] = masked_images[0].to(device)\n",
    "            masked_images[1] = masked_images[1].to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            prev_pred, prev_uncertainty = single_frame_model(masked_images[0])\n",
    "            model_input = torch.cat([masked_images[1], prev_pred, prev_uncertainty], dim=1).detach()\n",
    "            dehedged_predictions, uncertainty = model(model_input)\n",
    "            loss, mse = criterion(dehedged_predictions, uncertainty, ground_truths)\n",
    "\n",
    "            validation_loss.update(loss.item())\n",
    "            mse_meter.update(mse.mean().item())\n",
    "            uncertainty_meter.update(torch.mean(uncertainty))\n",
    "\n",
    "            # if i % print_freq == 0:\n",
    "            #     show_tensor_images([masked_images[0][0], dehedged_predictions[0], ground_truths[0]], [\"masked image\", \"pred\", \"ground truth\"])\n",
    "            #     logger.info('Epoch: [{0}][{1}/{2}]\\t'\n",
    "            #                 'Val Loss {loss.val:.5f} ({loss.avg:.5f})\\t'.format(epoch, i, len(val_loader), loss=validation_loss))\n",
    "\n",
    "        # log.txt info\n",
    "        show_tensor_images([masked_images[0][0], prev_pred[0], prev_uncertainty[0]], [\"prev masked image\", \"prev pred\", \"prev uncertainty\"], os.path.join(image_save_path, str(epoch) + \"_compare.png\"))\n",
    "        show_uncertainty_result(masked_images[1][0], dehedged_predictions[0], ground_truths[0], uncertainty[0], os.path.join(image_save_path, str(epoch) + \".png\"))\n",
    "\n",
    "        return validation_loss.avg, mse_meter.avg, uncertainty_meter.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(args):\n",
    "    checkpoint = args.checkpoint_load_path\n",
    "    image_save_path = os.path.join(args.checkpoint_save_path, \"images\")\n",
    "    make_dir(args.checkpoint_save_path)\n",
    "    make_dir(image_save_path)\n",
    "    make_dir(\"runs\")\n",
    "\n",
    "    writer = SummaryWriter(log_dir = os.path.join(\"runs\", args.tensorboard_fileName))\n",
    "    density_range = np.arange(args.min_density, args.max_density, 0.1)\n",
    "\n",
    "    train_dataset = StaticHedgedDataset(args.train_path, density_range, masked_img_per_item=args.masked_img_per_item, require_hedge_mask=False, color_mode=args.color_mode, debug=args.debug)\n",
    "    val_dataset = StaticHedgedDataset(args.val_path, density_range, masked_img_per_item=args.masked_img_per_item, require_hedge_mask=False, color_mode=args.color_mode, debug=args.debug)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=args.pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=args.pin_memory)\n",
    "    logger.info(\"CPU count = {}\".format(args.num_workers))\n",
    "    \n",
    "    if args.unet_type == \"unet\":\n",
    "        criterion = nn.MSELoss()\n",
    "    elif args.unet_type == \"uncertainty_unet\" or \"recurrent_uncertainty_unet\":\n",
    "        criterion = uncertainty_MSE_loss\n",
    "    else:\n",
    "        logger.info(\"model {} is not supported\".format(args.unet_type))\n",
    "\n",
    "    # init check point\n",
    "    if checkpoint is None:\n",
    "        if args.unet_type == \"unet\":\n",
    "            model = UNet(args.in_channel, args.out_channel).to(device)\n",
    "        elif args.unet_type == \"uncertainty_unet\" or args.unet_type == \"recurrent_uncertainty_unet\":\n",
    "            model = UncertaintyUNet(args.in_channel, args.out_channel, args.out_uncertainty_channel).to(device)\n",
    "            if args.unet_type == \"uncertainty_unet\" and args.pretrained_MSE_unet:\n",
    "                load_pretrained_mse_param(model, args.pretrained_MSE_unet, pretrained_fixed=False)\n",
    "        else:\n",
    "            logger.info(\"model {} is not supported\".format(args.unet_type))\n",
    "        \n",
    "        logger.info(\"training model type: {}\".format(args.unet_type))\n",
    "\n",
    "        start_epoch = 0\n",
    "        epochs_since_improvement = 0\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        \n",
    "        if args.optimizer == 'adam':\n",
    "            optimizer = OptimizerWrapper(torch.optim.Adam(model.parameters(), lr=args.lr))\n",
    "        elif args.optimizer == 'SGD':\n",
    "            optimizer = OptimizerWrapper(torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=args.nesterov))\n",
    "        elif args.optimizer == 'madgrad':\n",
    "            optimizer = OptimizerWrapper(madgrad.MADGRAD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, eps=args.eps))\n",
    "        else:\n",
    "            raise TypeError('optimizer {} is not supported.'.format(args.optimizer))\n",
    "\n",
    "        logger.info(\"using {} as optimizer, initial learning rate = {}\".format(args.optimizer, args.lr))\n",
    "\n",
    "        if args.scheduler:\n",
    "            scheduler = get_scheduler(optimizer, len(train_loader), start_epoch)\n",
    "        else:\n",
    "            scheduler = None\n",
    "            logger.info(\"not using any scheduler\")        \n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        model = checkpoint['u_net_model']\n",
    "        \n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        epochs_since_improvement = checkpoint['epoch_since_improvement']\n",
    "        optimizer = checkpoint['optimizer']\n",
    "        if args.change_scheduler:\n",
    "            logger.info(\"change scheduler to {}\".format(args.scheduler))\n",
    "            scheduler = get_scheduler(optimizer, len(train_loader), start_epoch)\n",
    "        else:\n",
    "            scheduler = checkpoint['scheduler']\n",
    "            if args.scheduler == 'onecycle':\n",
    "                scheduler = OneCycleLR(optimizer.optimizer, max_lr=args.max_lr, steps_per_epoch=len(train_loader), epochs=args.end_epoch - start_epoch)\n",
    "        best_loss = checkpoint['best_loss']\n",
    "\n",
    "        logger.info(\"continue training from ep{}, best loss so far is {}\".format(start_epoch, best_loss))\n",
    "\n",
    "        if args.change_lr:\n",
    "            logger.info(\"adjust lr to \", args.lr)\n",
    "            optimizer.adjust_lr(args.lr)\n",
    "        \n",
    "    model = get_data_parallel_model(model)\n",
    "    \n",
    "    if args.unet_type == \"unet\":\n",
    "        unet_training(start_epoch, args.end_epoch, train_loader, val_loader, model, criterion, optimizer, image_save_path, logger, writer, scheduler, best_loss, epochs_since_improvement)\n",
    "    elif args.unet_type == \"uncertainty_unet\":\n",
    "        uncertainty_unet_training(start_epoch, args.end_epoch, train_loader, val_loader, model, criterion, optimizer, image_save_path, logger, writer, scheduler, best_loss, epochs_since_improvement)\n",
    "    elif args.unet_type == \"recurrent_uncertainty_unet\":\n",
    "        single_unet_model = get_data_parallel_model(load_checkpoint_model(args.pretrained_uncertainty_unet))\n",
    "        single_unet_model.eval()\n",
    "        recurrent_uncertainty_unet_training(start_epoch, args.end_epoch, train_loader, val_loader, model, single_unet_model, criterion, optimizer, image_save_path, logger, writer, scheduler, best_loss, epochs_since_improvement)    \n",
    "\n",
    "if train_flag:\n",
    "    args = parse_args()\n",
    "    model_training(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /tmp/ipykernel_31536/4134605674.py in <module>\n",
    "#      91 if train_flag:\n",
    "#      92     args = parse_args()\n",
    "# ---> 93     model_training(args)\n",
    "\n",
    "# /tmp/ipykernel_31536/4134605674.py in model_training(args)\n",
    "#      81 \n",
    "#      82     if args.unet_type == \"unet\":\n",
    "# ---> 83         unet_training(start_epoch, args.end_epoch, train_loader, val_loader, model, criterion, optimizer, image_save_path, logger, writer, scheduler, best_loss, epochs_since_improvement)\n",
    "#      84     elif args.unet_type == \"uncertainty_unet\":\n",
    "#      85         uncertainty_unet_training(start_epoch, args.end_epoch, train_loader, val_loader, model, criterion, optimizer, image_save_path, logger, writer, scheduler, best_loss, epochs_since_improvement)\n",
    "\n",
    "# /tmp/ipykernel_31536/585321851.py in unet_training(start_epoch, end_epoch, train_loader, val_loader, model, criterion, optimizer, image_save_path, logger, writer, scheduler, best_loss, epochs_since_improvement)\n",
    "#       1 def unet_training(start_epoch, end_epoch, train_loader, val_loader, model, criterion, optimizer, image_save_path, logger, writer, scheduler, best_loss, epochs_since_improvement):\n",
    "#       2     for ep in range(start_epoch, end_epoch):\n",
    "# ----> 3         train_loss = train_unet(train_loader, model, criterion, optimizer, args.clip_val, ep, scheduler)\n",
    "#       4         val_loss = val_unet(val_loader, model, criterion, ep, image_save_path)\n",
    "#       5         log_and_record(logger, writer, train_loss, val_loss, ep)\n",
    "\n",
    "# /tmp/ipykernel_31536/585321851.py in train_unet(train_loader, model, criterion, optimizer, clip_val, epoch, scheduler)\n",
    "#      39     training_loss = AverageMeter()\n",
    "#      40 \n",
    "# ---> 41     for i, (masked_images, ground_truths, _,  _) in enumerate(train_loader):\n",
    "#      42         ground_truths = ground_truths.to(device)\n",
    "#      43         masked_images[0] = masked_images[0].to(device)\n",
    "\n",
    "# /opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)\n",
    "#     519             if self._sampler_iter is None:\n",
    "#     520                 self._reset()\n",
    "# --> 521             data = self._next_data()\n",
    "#     522             self._num_yielded += 1\n",
    "#     523             if self._dataset_kind == _DatasetKind.Iterable and \\\n",
    "\n",
    "# /opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)\n",
    "#    1201             else:\n",
    "#    1202                 del self._task_info[idx]\n",
    "# -> 1203                 return self._process_data(data)\n",
    "#    1204 \n",
    "#    1205     def _try_put_index(self):\n",
    "\n",
    "# /opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _process_data(self, data)\n",
    "#    1227         self._try_put_index()\n",
    "#    1228         if isinstance(data, ExceptionWrapper):\n",
    "# -> 1229             data.reraise()\n",
    "#    1230         return data\n",
    "#    1231 \n",
    "\n",
    "# /opt/conda/lib/python3.7/site-packages/torch/_utils.py in reraise(self)\n",
    "#     423             # have message field\n",
    "#     424             raise self.exc_type(message=msg)\n",
    "# --> 425         raise self.exc_type(msg)\n",
    "#     426 \n",
    "#     427 \n",
    "\n",
    "# UnidentifiedImageError: Caught UnidentifiedImageError in DataLoader worker process 2.\n",
    "# Original Traceback (most recent call last):\n",
    "#   File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n",
    "#     data = fetcher.fetch(index)\n",
    "#   File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
    "#     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
    "#   File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
    "#     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
    "#   File \"/tmp/ipykernel_31536/3801022188.py\", line 70, in __getitem__\n",
    "#     masked_images.append(self.read_img(masked_image_path))\n",
    "#   File \"/tmp/ipykernel_31536/3801022188.py\", line 37, in read_img\n",
    "#     im = np.array(Image.open(path).convert(\"RGB\"))\n",
    "#   File \"/opt/conda/lib/python3.7/site-packages/PIL/Image.py\", line 3031, in open\n",
    "#     \"cannot identify image file %r\" % (filename if filename else fp)\n",
    "# PIL.UnidentifiedImageError: cannot identify image file 'images/hedged_images/train/794/ILSVRC2012_val_00000527/0.8/0.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoint/unet(madgrad+onecycle)/ep300(lr=0.001)/checkpoint.tar\"\n",
    "checkpoint = torch.load(checkpoint)\n",
    "optimizer = checkpoint['optimizer']\n",
    "learning_rate = optimizer.optimizer.param_groups[0]['lr']\n",
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "\n",
    "print(\"total\", t)\n",
    "print(\"reserved\", r)\n",
    "print(\"allocated\", a)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KX9D6vunFx4X",
    "oVbQyGw-7aNB",
    "qqzI1CHHGFpF",
    "cNNya4AOGOhs",
    "rA3yWnVxbZvw",
    "Xr1H3giKnH1n",
    "nEsfvs3lncRw",
    "iWkx_5LUn2na",
    "bw4V977qKMtO",
    "FRHTo4VcBcCR",
    "IUWKZeUhEYwt",
    "gX4aPEeSD7ts",
    "oQeZchOQALYR",
    "YEDawdlsAO9f"
   ],
   "name": "Image_Dehedger.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
